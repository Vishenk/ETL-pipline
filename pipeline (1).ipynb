{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Импорты и инициализация спарк сесии"
      ],
      "metadata": {
        "id": "4o1-7xmgulk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions  as fnct\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.conf import SparkConf\n",
        "import os\n",
        "import logging"
      ],
      "metadata": {
        "id": "u7KexAAUB4PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "spark = (SparkSession.builder.master('local').appName('Spark').config(conf=SparkConf()).getOrCreate())\n",
        "\n",
        "updates=spark.read.parquet('/content/updates.parquet', header =True, inferSchema=True)\n",
        "\n",
        "sources=spark.read.parquet('/content/sources.parquet', header =True, inferSchema=True)\n",
        "\n",
        "client_attribute=spark.read.parquet('/content/client_attribute.parquet', header =True, inferSchema=True)\n",
        "# добавляем конструктор логов, который будет записывать все логи с уровнем INFO и выше в файл \"py_log.log\". Каждый раз новые логи присоединяются к текущим\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO, force=True, filename='/content/py_log.log',filemode=\"a\",\n",
        "                    format=\"%(asctime)s %(levelname)s %(message)s\")\n",
        "\n",
        "try:\n",
        "  monthly_report=spark.read.parquet('/content/monthly_report', header =True, inferSchema=True).show()\n",
        "except:\n",
        "  schema = StructType([\n",
        "    StructField('client_id',\n",
        "                IntegerType(), True),\n",
        "    StructField('id_attribute',\n",
        "                IntegerType(), True),\n",
        "    StructField('name',\n",
        "                StringType(), True),\n",
        "    StructField('dtype',\n",
        "                StringType(), True),\n",
        "    StructField('value',\n",
        "                StringType(), True),\n",
        "    StructField('report_dt',\n",
        "                StringType(), True)\n",
        "  ])\n",
        "  monthly_report=spark.createDataFrame([], schema)\n",
        "\n",
        "try:\n",
        "  daily_report=spark.read.parquet('/content/daily_report', header =True, inferSchema=True).show()\n",
        "except:\n",
        "  schema = StructType([\n",
        "    StructField('client_id',\n",
        "                IntegerType(), True),\n",
        "    StructField('id_attribute',\n",
        "                IntegerType(), True),\n",
        "    StructField('name',\n",
        "                StringType(), True),\n",
        "    StructField('dtype',\n",
        "                StringType(), True),\n",
        "    StructField('value',\n",
        "                StringType(), True),\n",
        "    StructField('row_actual_from',\n",
        "                StringType(), True),\n",
        "    StructField('row_actual_to',\n",
        "                StringType(), True)\n",
        "  ])\n",
        "  daily_report=spark.createDataFrame([], schema)"
      ],
      "metadata": {
        "id": "xcZq4wcaucf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# подсчет количества источников\n",
        "list_of_files=os.listdir('/content/sources')\n",
        "if len(list_of_files)>3: # если количество не совпадает, сообщаем от необходимости запросить документацию и пересмотреть пайплайн забора данных\n",
        "  logging.error('New sources was added. Ask for documentation ')\n",
        "else:\n",
        "  logging.info('All ' + str(len(list_of_files)) + ' sources was read successfully')"
      ],
      "metadata": {
        "id": "_-0-Rcs0N8ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_files.remove('.ipynb_checkpoints')"
      ],
      "metadata": {
        "id": "Mla8eeORAUd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Инициализация функций"
      ],
      "metadata": {
        "id": "rv1OXHP5-XMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# функции очистки/проверки данных\n",
        "def check_types_deb(data):\n",
        "  mydic = dict({\"client_id\": \"int\",\n",
        "       \"onl_bank_active_1m_nflag\": \"int\",\n",
        "       \"auto_pay_active_qty\": \"int\",\n",
        "       \"cl_income_1m_amt\": \"decimal(18, 2)\",\n",
        "       \"dep_acc_1st_open_dt\": \"timestamp\",\n",
        "       \"wdr_cash_6m_amt\": \"decimal(18, 2)\",\n",
        "       \"cash_op_6m_amt\": \"decimal(18, 2)\",\n",
        "       \"cash_3m_qty\": \"decimal(18, 2)\",\n",
        "       \"lst_balance_amt\": \"decimal(18, 2)\",\n",
        "       \"card_active_1m_nflag\": \"int\",\n",
        "       \"row_update_dtime\": \"timestamp\",\n",
        "       \"row_loading_id\": \"bigint\",\n",
        "       \"row_hash_val\": \"string\",\n",
        "       \"report_dt\": \"string\"})\n",
        "  for key in mydic:\n",
        "    if dict(data.dtypes)[key] != mydic.get(key):\n",
        "     try:\n",
        "        data = data.withColumn(key, data[key].cast(mydic.get(key)))\n",
        "     except Error:\n",
        "       logging.error(\"failed to cast source columns to correct types\",exc_info=True)\n",
        "  return (data)\n",
        "def check_types_credit(data):\n",
        "  mydic = dict({\"client_id\": \"int\",\n",
        "                \"client_income_amt\": \"int\",\n",
        "                \"oi_total_amt\": \"int\",\n",
        "                \"act_pl_os_rub_amt\": \"decimal(18, 2)\",\n",
        "                \"payroll_client_nflag\": \"int\",\n",
        "                \"inf_payroll_rub_amt\": \"decimal(18, 2)\",\n",
        "                \"legal_entity_amt\": \"decimal(18, 2)\",\n",
        "                \"otf_loan_rub_amt\": \"decimal(18, 2)\",\n",
        "                \"otf_fee_rub_amt\": \"decimal(18, 2)\",\n",
        "                \"inf_transfer_rub_amt\": \"decimal(18, 2)\",\n",
        "                \"cc_ever_nflag\": \"int\", \"row_update_dtime\":\n",
        "                \"timestamp\", \"row_loading_id\": \"bigint\",\n",
        "                \"row_hash_val\": \"string\", \"report_dt\": \"string\"})\n",
        "  for key in mydic:\n",
        "    if dict(data.dtypes)[key] != mydic.get(key):\n",
        "     try:\n",
        "        data = data.withColumn(key, data[key].cast(mydic.get(key)))\n",
        "     except Error:\n",
        "       logging.error(\"failed to cast source columns to correct types\",exc_info=True)\n",
        "  return (data)\n",
        "def check_types_client(data):\n",
        "  mydic = dict({\"client_id\": \"int\",\n",
        "                \"srv_mb_nflag\": \"int\",\n",
        "                \"cc_stoplist\": \"tinyint\",\n",
        "                \"lne_tot_debt_int_ovrd_rub_amt\": \"decimal(18, 2)\",\n",
        "                \"lne_tot_debt_ovrd_rub_amt\": \"decimal(18, 2)\",\n",
        "                \"row_update_dtime\": \"timestamp\",\n",
        "                \"row_loading_id\": \"bigint\",\n",
        "                \"row_hash_val\": \"string\",\n",
        "                \"row_actual_from\": \"string\",\n",
        "                \"row_actual_to\": \"string\"})\n",
        "  for key in mydic:\n",
        "    if dict(data.dtypes)[key] != mydic.get(key):\n",
        "     try:\n",
        "        data = data.withColumn(key, data[key].cast(mydic.get(key)))\n",
        "     except Error:\n",
        "       logging.error(\"failed to cast source columns to correct types\",exc_info=True)\n",
        "  return (data)\n",
        "\n",
        "#проверка типов\n",
        "def check_types(source, name):\n",
        "  if name=='deb_cards_info':\n",
        "    return(check_types_deb(source))\n",
        "  elif name=='credit_cards_info':\n",
        "    return(check_types_credit(source))\n",
        "  else:\n",
        "    return(check_types_client(source))\n",
        "\n",
        "#общая проверка\n",
        "def check_data(data, name):\n",
        "  # Убрать строки с null в client_id или с null во всех атрибутах кроме client_id\n",
        "  condition=''\n",
        "  columns=data.columns\n",
        "  columns.pop(0)\n",
        "  for attribute in columns:\n",
        "    condition+='and '+attribute+' is null '\n",
        "  condition=condition[4:]\n",
        "  clean_data_prep=data.where('client_id is not null and not ('+condition+ ')')\n",
        "  # проверить соответствие типов и преобразовать к кооректным\n",
        "  clean_data=check_types(clean_data_prep, name)\n",
        "  return(clean_data)"
      ],
      "metadata": {
        "id": "uhhhTM3eCMWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Процесс забора и очистки данных"
      ],
      "metadata": {
        "id": "F3uV5opy-Lg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# запустить процесс для каждого источника\n",
        "for name in list_of_files:\n",
        "  source=spark.read.parquet('/content/sources/'+name, header =True, inferSchema=True)\n",
        "  # проверяем по какой колонке партиционирован источник\n",
        "  if sources.select(sources.partitioned_by).where(sources.name==name).collect()[0][0]=='report_dt': # есть, работаем с scd1\n",
        "\n",
        "    #Есть строки с report_dt источника > max(report_dt) таблицы monthly_report?\n",
        "    max_dt=source.selectExpr(\"max(report_dt)\").collect()[0][0]\n",
        "    our_max_dt=monthly_report.where(monthly_report.name.isin(source.columns)).select(fnct.col('report_dt')).agg(fnct.max(\"report_dt\").alias('our_max_dt'))\n",
        "    if our_max_dt.collect()[0][0] is None or max_dt>our_max_dt.collect()[0][0]: #есть\n",
        "\n",
        "      #какие новые партиции появились?\n",
        "      if our_max_dt.collect()[0][0] is None: #если первый раз забираем данные из источника\n",
        "        new_partitions_dt=source.select('report_dt').distinct().collect()\n",
        "      else: #если не первый\n",
        "        new_partitions_dt=source.select('report_dt').where(source.report_dt>our_max_dt.collect()[0][0]).distinct().collect()\n",
        "      #запускаем процесс для каждой новой партиции\n",
        "      for partition_dt in new_partitions_dt:\n",
        "        #берем новые данные\n",
        "        new_data_raw=source.where(source.report_dt==partition_dt[0])\n",
        "        #очищаем данные\n",
        "        new_data=check_data(new_data_raw, name)\n",
        "\n",
        "        #преобразуем данные в строчный тип\n",
        "        attribute_cols=new_data.columns\n",
        "        new_data_casted=new_data.select([fnct.col(c).cast(\"string\") for c in attribute_cols])\n",
        "\n",
        "        #переворачиваем новые данные в вертикальный формат\n",
        "        for tech_column in ['row_update_dtime', 'row_loading_id', 'row_hash_val']:\n",
        "          attribute_cols.remove(tech_column)\n",
        "        stack_expr = \"stack({num}, {cols})\".format(\n",
        "          num=len(attribute_cols),\n",
        "          cols=\", \".join([f\"'{a}', {a}\" for a in attribute_cols])\n",
        "        )\n",
        "        new_data_vert=new_data_casted.select('client_id', 'report_dt', fnct.expr(stack_expr))\n",
        "\n",
        "        #дополняем данные столбцами id_attribute, dtype\n",
        "        new_data_final=new_data_vert.join(client_attribute, new_data_vert.col0==client_attribute.name, 'left').where('id_attribute is not null').selectExpr('client_id', 'id_attribute', 'col0 as name', 'dtype', 'col1 as value', 'report_dt')\n",
        "\n",
        "        #добавляем новые данные в конечную таблицу\n",
        "        monthly_report=monthly_report.unionAll(new_data_final)\n",
        "        logging.info('Partition '+str(partition_dt[0])+' succesfully inserted from source ' + name)\n",
        "\n",
        "        #обновляем журнал обновлений\n",
        "        attribute_cols=new_data.columns\n",
        "        for tech_column in ['report_dt', 'client_id', 'row_loading_id', 'row_hash_val']:\n",
        "          attribute_cols.remove(tech_column)\n",
        "        stack_expr = \"stack({num}, {cols})\".format(\n",
        "          num=len(attribute_cols),\n",
        "          cols=\", \".join([f\"'{a}', {a}\" for a in attribute_cols])\n",
        "        )\n",
        "        new_data_vert=new_data_casted.select('row_update_dtime', fnct.expr(stack_expr))\n",
        "        id_source_df=sources.select('id_source').where(sources.name==name)\n",
        "\n",
        "        new_updates=new_data_vert.join(client_attribute, new_data_vert.col0==client_attribute.name, 'left').join(id_source_df, how='cross').where('id_attribute is not null').selectExpr('id_attribute', 'id_source', 'row_update_dtime').groupBy('id_attribute', 'id_source').agg(fnct.max('row_update_dtime').alias('update_dt_sources'))\n",
        "        updates=updates.unionAll(new_updates).groupBy('id_attribute', 'id_source').agg(fnct.max('update_dt_sources').alias('update_dt_sources'))\n",
        "\n",
        "    else: #нет\n",
        "      logging.info('No new data in source' + name)\n",
        "\n",
        "  else: # нет, работаем с scd2\n",
        "    #Есть строки с row_actual_to источника = update_dt_sources таблицы обновлений?\n",
        "    max_dt=source.where(\"row_actual_to!='9999-01-01 00:00:00'\").selectExpr(\"max(row_actual_to)\").collect()[0][0]\n",
        "    our_max_dt=daily_report.where(daily_report.name.isin(source.columns)).where(\"row_actual_to!='9999-01-01 00:00:00'\").select(fnct.col('row_actual_to')).agg(fnct.max(\"row_actual_to\").alias('our_max_dt'))\n",
        "    if our_max_dt.collect()[0][0] is None or max_dt>our_max_dt.collect()[0][0]: #есть\n",
        "\n",
        "      #собираем все новые партиции?\n",
        "      if our_max_dt.collect()[0][0] is None: #если первый раз забираем данные из источника\n",
        "        new_partitions_dt=source.select('row_actual_to').distinct().collect()\n",
        "      else: #если не первый\n",
        "        new_partitions_dt=source.where(source.row_actual_to>our_max_dt.collect()[0][0]).select('row_actual_to').distinct().collect()\n",
        "\n",
        "      #запускаем процесс для каждой новой партиции\n",
        "      for partition_dt in new_partitions_dt:\n",
        "        #берем новые данные\n",
        "        new_data_raw=source.where(source.row_actual_to==partition_dt[0])\n",
        "        #очищаем данные\n",
        "        new_data=check_data(new_data_raw, name)\n",
        "\n",
        "        #преобразуем данные в строчный тип\n",
        "        attribute_cols=new_data.columns\n",
        "        new_data_casted=new_data.select([fnct.col(c).cast(\"string\") for c in attribute_cols])\n",
        "\n",
        "        #переворачиваем новые данные в вертикальный формат\n",
        "        for tech_column in ['row_update_dtime', 'row_loading_id', 'row_hash_val']:\n",
        "          attribute_cols.remove(tech_column)\n",
        "        stack_expr = \"stack({num}, {cols})\".format(\n",
        "          num=len(attribute_cols),\n",
        "          cols=\", \".join([f\"'{a}', {a}\" for a in attribute_cols])\n",
        "        )\n",
        "        new_data_vert=new_data_casted.select('client_id', 'row_actual_from', 'row_actual_to', fnct.expr(stack_expr))\n",
        "        #дополняем данные столбцами id_attribute, dtype\n",
        "        new_data_final=new_data_vert.join(client_attribute, new_data_vert.col0==client_attribute.name, 'left').where('id_attribute is not null').selectExpr('client_id', 'id_attribute', 'col0 as name', 'dtype', 'col1 as value', 'row_actual_from', 'row_actual_to')\n",
        "\n",
        "        #добавляем новые данные в конечную таблицу\n",
        "        daily_report=daily_report.where(\"row_actual_to!='9999-01-01 00:00:00'\").unionAll(new_data_final).groupBy('client_id', 'id_attribute', 'name', 'dtype', 'value').agg(fnct.min('row_actual_from').alias('row_actual_from'), fnct.max('row_actual_to').alias('row_actual_to'))\n",
        "        logging.info('Partition '+str(partition_dt[0])+' succesfully inserted from source ' + name)\n",
        "\n",
        "        #обновляем журнал обновлений\n",
        "        attribute_cols=new_data.columns\n",
        "        for tech_column in ['row_actual_to', 'row_actual_from', 'row_loading_id', 'row_hash_val']:\n",
        "          attribute_cols.remove(tech_column)\n",
        "        stack_expr = \"stack({num}, {cols})\".format(\n",
        "          num=len(attribute_cols),\n",
        "          cols=\", \".join([f\"'{a}', {a}\" for a in attribute_cols])\n",
        "        )\n",
        "        new_data_vert=new_data_casted.select('row_update_dtime', 'client_id', fnct.expr(stack_expr))\n",
        "        try:\n",
        "          new_updates=new_updates.unionAll(new_data_vert)\n",
        "        except:\n",
        "          new_updates=new_data_vert\n",
        "      id_source_df=sources.select('id_source').where(sources.name==name)\n",
        "      new_updates=new_updates.groupBy('client_id', 'col0', 'col1').agg(fnct.min('row_update_dtime').alias('row_update_dtime')).groupBy('col0').agg(fnct.max('row_update_dtime').alias('row_update_dtime')).select('row_update_dtime', 'col0').join(client_attribute, new_updates.col0==client_attribute.name, 'inner').join(id_source_df, how='cross').where('id_attribute is not null').selectExpr('id_attribute', 'id_source', 'row_update_dtime')\n",
        "      updates=updates.unionAll(new_updates).groupBy('id_attribute', 'id_source').agg(fnct.max('update_dt_sources').alias('update_dt_sources'))\n",
        "\n",
        "    else: #нет\n",
        "      logging.info('No new data in source' + name)"
      ],
      "metadata": {
        "id": "z8g1cJ7bN_tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_report.write.mode(\"overwrite\").parquet(\"/content/results/monthly_report\", \"overwrite\", 'report_dt')\n",
        "daily_report.write.mode(\"overwrite\").parquet(\"/content/results/daily_report\", \"overwrite\", 'row_actual_to')"
      ],
      "metadata": {
        "id": "198NCFB1wurn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Показываем результат"
      ],
      "metadata": {
        "id": "AdXWCpIzr18d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_report.show(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t12vIwg-2KWK",
        "outputId": "418da2f6-2a35-448e-912e-38ca504c0c15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+--------------------+--------------+-------------------+-------------------+\n",
            "|client_id|id_attribute|                name|         dtype|              value|          report_dt|\n",
            "+---------+------------+--------------------+--------------+-------------------+-------------------+\n",
            "|        1|          15|onl_bank_active_1...|           int|                  1|2024-11-24 00:00:00|\n",
            "|        1|          16| auto_pay_active_qty|           int|                  1|2024-11-24 00:00:00|\n",
            "|        1|          17|    cl_income_1m_amt|decimal(18, 2)|               1.00|2024-11-24 00:00:00|\n",
            "|        1|          18| dep_acc_1st_open_dt|     timestamp|2024-11-24 00:00:00|2024-11-24 00:00:00|\n",
            "|        1|          19|     wdr_cash_6m_amt|decimal(18, 2)|               1.00|2024-11-24 00:00:00|\n",
            "|        1|          20|      cash_op_6m_amt|decimal(18, 2)|               1.00|2024-11-24 00:00:00|\n",
            "|        1|          21|         cash_3m_qty|decimal(18, 2)|               1.00|2024-11-24 00:00:00|\n",
            "|        1|          22|     lst_balance_amt|decimal(18, 2)|               1.00|2024-11-24 00:00:00|\n",
            "|        1|          23|card_active_1m_nflag|           int|                  1|2024-11-24 00:00:00|\n",
            "|        2|          15|onl_bank_active_1...|           int|                  1|2024-11-24 00:00:00|\n",
            "|        2|          16| auto_pay_active_qty|           int|                  1|2024-11-24 00:00:00|\n",
            "|        2|          17|    cl_income_1m_amt|decimal(18, 2)|               1.00|2024-11-24 00:00:00|\n",
            "|        2|          18| dep_acc_1st_open_dt|     timestamp|2024-11-24 00:00:00|2024-11-24 00:00:00|\n",
            "|        2|          19|     wdr_cash_6m_amt|decimal(18, 2)|               1.00|2024-11-24 00:00:00|\n",
            "|        2|          20|      cash_op_6m_amt|decimal(18, 2)|               1.00|2024-11-24 00:00:00|\n",
            "|        2|          21|         cash_3m_qty|decimal(18, 2)|               1.00|2024-11-24 00:00:00|\n",
            "|        2|          22|     lst_balance_amt|decimal(18, 2)|               1.00|2024-11-24 00:00:00|\n",
            "|        2|          23|card_active_1m_nflag|           int|                  1|2024-11-24 00:00:00|\n",
            "|        1|           5|   client_income_amt|           int|                  1|2024-11-24 00:00:00|\n",
            "|        1|           6|        oi_total_amt|           int|                  1|2024-11-24 00:00:00|\n",
            "|        1|           7|   act_pl_os_rub_amt|decimal(18, 2)|               1.00|2024-11-24 00:00:00|\n",
            "|        1|           8|payroll_client_nflag|           int|                  1|2024-11-24 00:00:00|\n",
            "|        1|           9| inf_payroll_rub_amt|decimal(18, 2)|               1.00|2024-11-24 00:00:00|\n",
            "|        1|          10|    legal_entity_amt|decimal(18, 2)|               1.00|2024-11-24 00:00:00|\n",
            "|        1|          11|    otf_loan_rub_amt|decimal(18, 2)|               1.00|2024-11-24 00:00:00|\n",
            "|        1|          12|     otf_fee_rub_amt|decimal(18, 2)|               1.00|2024-11-24 00:00:00|\n",
            "|        1|          13|inf_transfer_rub_amt|decimal(18, 2)|               1.00|2024-11-24 00:00:00|\n",
            "|        1|          14|       cc_ever_nflag|           int|                  1|2024-11-24 00:00:00|\n",
            "|        2|           5|   client_income_amt|           int|                  1|2024-11-24 00:00:00|\n",
            "|        2|           6|        oi_total_amt|           int|                  1|2024-11-24 00:00:00|\n",
            "|        2|           7|   act_pl_os_rub_amt|decimal(18, 2)|               1.00|2024-11-24 00:00:00|\n",
            "|        2|           8|payroll_client_nflag|           int|                  1|2024-11-24 00:00:00|\n",
            "|        2|           9| inf_payroll_rub_amt|decimal(18, 2)|               1.00|2024-11-24 00:00:00|\n",
            "|        2|          10|    legal_entity_amt|decimal(18, 2)|               1.00|2024-11-24 00:00:00|\n",
            "|        2|          11|    otf_loan_rub_amt|decimal(18, 2)|               1.00|2024-11-24 00:00:00|\n",
            "|        2|          12|     otf_fee_rub_amt|decimal(18, 2)|               1.00|2024-11-24 00:00:00|\n",
            "|        2|          13|inf_transfer_rub_amt|decimal(18, 2)|               1.00|2024-11-24 00:00:00|\n",
            "|        2|          14|       cc_ever_nflag|           int|                  1|2024-11-24 00:00:00|\n",
            "+---------+------------+--------------------+--------------+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "daily_report.show(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7OoJpO6FSjU",
        "outputId": "afcedd33-64d5-43ff-ee89-a751c1554448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+--------------------+--------------+-----+-------------------+-------------------+\n",
            "|client_id|id_attribute|                name|         dtype|value|    row_actual_from|      row_actual_to|\n",
            "+---------+------------+--------------------+--------------+-----+-------------------+-------------------+\n",
            "|        1|           1|        srv_mb_nflag|           int|    1|2024-11-24 00:00:00|9999-01-01 00:00:00|\n",
            "|        1|           2|         cc_stoplist|       tinyint|    1|2024-11-24 00:00:00|9999-01-01 00:00:00|\n",
            "|        1|           3|lne_tot_debt_int_...|decimal(18, 2)| 1.00|2024-11-24 00:00:00|9999-01-01 00:00:00|\n",
            "|        1|           4|lne_tot_debt_ovrd...|decimal(18, 2)| 1.00|2024-11-24 00:00:00|9999-01-01 00:00:00|\n",
            "|        2|           1|        srv_mb_nflag|           int|    1|2024-11-24 00:00:00|9999-01-01 00:00:00|\n",
            "|        2|           2|         cc_stoplist|       tinyint|    1|2024-11-24 00:00:00|9999-01-01 00:00:00|\n",
            "|        2|           3|lne_tot_debt_int_...|decimal(18, 2)| 1.00|2024-11-24 00:00:00|2024-11-24 00:00:00|\n",
            "|        2|           3|lne_tot_debt_int_...|decimal(18, 2)| 2.00|2024-11-25 00:00:00|9999-01-01 00:00:00|\n",
            "|        2|           4|lne_tot_debt_ovrd...|decimal(18, 2)| 1.00|2024-11-24 00:00:00|9999-01-01 00:00:00|\n",
            "+---------+------------+--------------------+--------------+-----+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "updates.show(50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ3zxyLD_SlO",
        "outputId": "3cc5dbeb-3be7-41b4-a78a-88d12713d3ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+---------+-------------------+\n",
            "|id_attribute|id_source|  update_dt_sources|\n",
            "+------------+---------+-------------------+\n",
            "|           1|        3|2024-11-24 00:00:00|\n",
            "|           2|        3|2024-11-24 00:00:00|\n",
            "|           3|        3|2024-11-25 00:00:00|\n",
            "|           4|        3|2024-11-24 00:00:00|\n",
            "|           5|        2|2024-11-24 00:00:00|\n",
            "|           6|        2|2024-11-24 00:00:00|\n",
            "|           7|        2|2024-11-24 00:00:00|\n",
            "|           8|        2|2024-11-24 00:00:00|\n",
            "|           9|        2|2024-11-24 00:00:00|\n",
            "|          10|        2|2024-11-24 00:00:00|\n",
            "|          11|        2|2024-11-24 00:00:00|\n",
            "|          12|        2|2024-11-24 00:00:00|\n",
            "|          13|        2|2024-11-24 00:00:00|\n",
            "|          14|        2|2024-11-24 00:00:00|\n",
            "|          15|        1|2024-11-24 00:00:00|\n",
            "|          16|        1|2024-11-24 00:00:00|\n",
            "|          17|        1|2024-11-24 00:00:00|\n",
            "|          18|        1|2024-11-24 00:00:00|\n",
            "|          19|        1|2024-11-24 00:00:00|\n",
            "|          20|        1|2024-11-24 00:00:00|\n",
            "|          21|        1|2024-11-24 00:00:00|\n",
            "|          22|        1|2024-11-24 00:00:00|\n",
            "|          23|        1|2024-11-24 00:00:00|\n",
            "+------------+---------+-------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}